{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import csv\n",
    "import html\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "# Third-party library imports\n",
    "import ftfy\n",
    "import geonamescache\n",
    "import locationtagger\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib_venn import venn2\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import regex\n",
    "import importlib\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from joblib import Parallel, delayed\n",
    "from langdetect import detect_langs, LangDetectException, DetectorFactory\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Local application/library specific imports\n",
    "sys.path.insert(0, '../Utility files')\n",
    "import utils2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering, Preprocessing, and Language and Location Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This markdown contains the initial data cleaning and filters applied to the data.\n",
    "\n",
    "\n",
    "\n",
    "Markdown overview:\n",
    "1. Data Inspection and duplicate removal \n",
    "    - Summary statistics of users and brands.\n",
    "    - Any missing data, duplicates etc.\n",
    "\n",
    "2. Filters to match the SES embedding method:\n",
    "    On the marker-follower (edgelist) df:\n",
    "    - Remove users that follow less than 5 (or more) brands\n",
    "    - Match the Follower_Ids in the now filtered marker-follower df with the follower-bio df. As such, the follower bios will only include users that follow more than five brands. Subsequent filters will be on the correct users (up to date follower-bios).\n",
    "\n",
    "    On the follower-bios df:\n",
    "    - Remove users with less than 25 followers\n",
    "    - Remove users with less than 100 tweets\n",
    "    - Clean the description column (normalise, remove emojis etc.)\n",
    "\n",
    "3. Keep only french accounts\n",
    "    - Keep only users who have both french bios and a location that can be surely detected to be in France\n",
    "\n",
    "4. Create an informative edgelist to use further in the analyses\n",
    "- This involves gathering the marker types associated with each marker-folower pair.\n",
    "- I am making one for the full data and one for the french users only.\n",
    "\n",
    "\n",
    "Files created that will be used further on:\n",
    "- followers_bios_french_updated.csv : the df with followers and their bios. Followers are french, and all filters have been applied\n",
    "- french_edgelist_marker_threshold.csv: french edgelist with all filters applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data df of markers and their followers\n",
    "\n",
    "# Load markers-followers (later referred to as edgelist)\n",
    "load_path = '/home/livtollanes/SocialMarkers'\n",
    "file = 'markers_followers_2023-05-19.csv'\n",
    "file_path = os.path.join(load_path, file)\n",
    "\n",
    "req_cols = ['id', 'follower_id']\n",
    "dtypes = {'id': 'object',\n",
    "          'follower_id': 'object'}\n",
    "\n",
    "# Use pandas to load the csv file\n",
    "markers_followers = pd.read_csv(file_path, usecols=req_cols, dtype=dtypes)\n",
    "\n",
    "# Rename the twitter id column to follower id \n",
    "markers_followers.rename(columns={'id':'marker_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the followers bios and rename ID columns\n",
    "load_path = '/home/livtollanes/SocialMarkers'\n",
    "file = 'markers_followers_bios_2023-05-19.csv'\n",
    "file_path = os.path.join(load_path, file)\n",
    "\n",
    "req_cols = ['twitter_id', 'id', 'screen_name', 'description', 'location', 'tweets', 'followers', 'friends', 'likes', 'lists','timestamp_utc']\n",
    "\n",
    "dtypes = {\n",
    "    'twitter_id': 'object',\n",
    "    'id': 'object',\n",
    "    'screen_name': 'object',\n",
    "    'description': 'object',\n",
    "    'location': 'object',\n",
    "    'tweets': 'float64',\n",
    "    'followers': 'float64',\n",
    "    'friends': 'float64',\n",
    "    'witheld_in_countries': 'float64'\n",
    "}\n",
    "\n",
    "# Use pandas to load the csv file\n",
    "followers_bios = pd.read_csv(file_path, usecols=req_cols, dtype=dtypes)\n",
    "\n",
    "# Rename the twitter id column to follower id \n",
    "followers_bios.rename(columns={'twitter_id':'follower_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Inspection and Duplicate Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are 2,357,493 duplicate rows in the edgelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspection of the data\n",
    "utils2.summary_stats(followers_bios, print_dtypes=False)\n",
    "importlib.reload(utils2)\n",
    "utils2.summary_stats(markers_followers, print_dtypes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Duplicate inspection - do they look like duplicates?\n",
    "\n",
    "# # Sort the DataFrame to ensure that duplicates are next to each other\n",
    "# markers_followers_sorted = markers_followers.sort_values(by=list(markers_followers.columns))\n",
    "\n",
    "# # Find duplicates in the sorted DataFrame\n",
    "# duplicates = markers_followers_sorted[markers_followers_sorted.duplicated(keep=False)]\n",
    "\n",
    "# # Print the first 10 rows of duplicates (5 pairs)\n",
    "# print(duplicates.head(10))\n",
    "\n",
    "#drop the duplicates in markers_followers\n",
    "markers_followers.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Does the follower bio df contain the same followers as the edgelist?\n",
    "utils2.compare_column_values(followers_bios, markers_followers, 'follower_id') # Yes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filters to match the SES embedding method implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Filter the marker-follower df:\n",
    "    - Remove users that follow less than 5 (or more) brands\n",
    "    - Streamline the edgelist df and the follower bio df so that any filters applied to one df is reflected in the other\n",
    "\n",
    "- Filter the followers-bios df:\n",
    "    - Remove users witrh less than 25 followers\n",
    "    - Remove users with less than 100 tweets\n",
    "    - Clean the description column. (Remove emojis, weird characters etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Marker-follower df filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove users that follow less than 5 brands\n",
    "n = 5  # minimal number of brands followed required to be included in the analysis\n",
    "markers_followers_5 = utils2.filter_followers(df = markers_followers, follower_id_column = 'follower_id', min_brands= n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### What brands were removed in the process?\n",
    "\n",
    "# Get the unique marker_id values in the original and filtered DataFrames\n",
    "original_brands = set(markers_followers['marker_id'].unique())\n",
    "filtered_brands = set(markers_followers_5['marker_id'].unique())\n",
    "\n",
    "# Find the brands that are in the original DataFrame but not in the filtered DataFrame\n",
    "removed_brands = original_brands - filtered_brands\n",
    "\n",
    "# Print the removed brands\n",
    "print(\"Removed brands:\", removed_brands) #corresponds to \"Napapijiri97\", which kinda sounds like a fake profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamline the follower bio df to be streamlined with the filter applied to the edgelist\n",
    "followers_bios_5 = utils2.streamline_IDs(source = markers_followers_5, df_tofilter= followers_bios, column = 'follower_id')\n",
    "\n",
    "utils2.compare_column_values(followers_bios_5, markers_followers_5, 'follower_id')   # The two dfs are streamlined so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Follower-bios filtering:\n",
    "- Remove users with less than 25 followers\n",
    "- Remove users with less than 100 tweets\n",
    "- Update the markers-followers df to match the now filtered bio df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum of 25 followers and 100 tweets\n",
    "followers_bios_fullfilter = utils2.filter_by_tweets_and_followers(followers_bios_5, min_followers= 25, min_tweets= 100)\n",
    "\n",
    "\n",
    "#Streamline dfs again\n",
    "markers_followers_fullfilter = utils2.streamline_IDs(source= followers_bios_fullfilter, df_tofilter=markers_followers_5, column='follower_id')\n",
    "\n",
    "utils2.compare_column_values(followers_bios_fullfilter, markers_followers_fullfilter , 'follower_id') # dfs are streamlined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils2.summary_stats(markers_followers_fullfilter, print_dtypes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean description column to avoid writing problems\n",
    "followers_bios_fullfilter = utils2.process_description(followers_bios_fullfilter, 'description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #Now write the two dfs to csvs to save them in case something happens\n",
    "# markers_followers_fullfilter.to_csv('/home/livtollanes/NewData/initial_dfs/markers_followers_nolang.csv.csv', encoding='utf-8', index=False)\n",
    "\n",
    "# followers_bios_fullfilter.to_csv('/home/livtollanes/NewData/initial_dfs/followers_bios_nolang.csv', sep=',', encoding='utf-8', index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Language Filtering: keep only french accounts\n",
    "- We only want french language bios to be included in further analyses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data and double check for errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load the filtered edgelist \n",
    "# full_path1 = '/home/livtollanes/NewData/markers_followers_nolang.csv'\n",
    "# req_cols = ['marker_id', 'follower_id']\n",
    "# dtypes = {'marker_id': 'object',\n",
    "#           'follower_id': 'object'}\n",
    "\n",
    "# markers_followers = pd.read_csv(full_path1, encoding='utf-8', dtype=dtypes, usecols=req_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #Loading the cleaned and filtered followers bios \n",
    "# full_path = '/home/livtollanes/NewData/initial_dfs/followers_bios_nolang.csv'\n",
    "\n",
    "# req_cols = ['follower_id', 'screen_name', 'description', 'description_cleantext', 'location', 'tweets', 'followers', 'friends', 'likes', 'lists','timestamp_utc']\n",
    "\n",
    "# dtypes = {\n",
    "#     'follower_id': 'object',\n",
    "#     'screen_name': 'object',\n",
    "#     'description': 'object',\n",
    "#     'description_cleantext': 'object',\n",
    "#     'location': 'object',\n",
    "#     'tweets': 'float64',\n",
    "#     'followers': 'float64',\n",
    "#     'friends': 'float64'\n",
    "# }\n",
    "\n",
    "# follower_bios = pd.read_csv(full_path, usecols=req_cols, dtype=dtypes, engine= 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils2.compare_column_values(follower_bios, markers_followers, 'follower_id') #The follower_ids are still streamlined, indicating that writing and reading of the cleaned dfs was successful\n",
    "\n",
    "#summary stats on the dfs again, to make sure that no strange things have happened during the cleaning process\n",
    "utils2.summary_stats(follower_bios, print_dtypes= False)\n",
    "utils2.summary_stats(markers_followers, print_dtypes= False)\n",
    "\n",
    "#everything looks fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The language detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The language detection is performed with the langdetect package. This package is based on Google's language detection API. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the DataFrame for each function\n",
    "follower_bios_copy = follower_bios.copy()\n",
    "\n",
    "# Use the copied DataFrame in the functions. Add and detect language to the lang df\n",
    "lang = utils2.add_and_detect_language(follower_bios_copy, 'description_cleantext', seed = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Language stats\n",
    "utils2.calculate_language_percentages(lang, 'language') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of number of users with bios and locations\n",
    "utils2.location_bio_stats(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the intersection of users with bios and locations\n",
    "\n",
    "# Calculate the number of users for each category\n",
    "total_users = lang.shape[0]\n",
    "users_with_location = lang['location'].notna().sum()\n",
    "users_with_bios = lang['description_cleantext'].notna().sum()\n",
    "users_with_both = lang[(lang['location'].notna()) & (lang['description_cleantext'].notna())].shape[0]\n",
    "\n",
    "# Create the Venn diagram with custom colors\n",
    "v = venn2(subsets=(users_with_location, users_with_bios, users_with_both),\n",
    "    set_labels=('Users with location data', 'Users with bios'),\n",
    "    set_colors=('purple', 'orange'))\n",
    "\n",
    "# Calculate the percentages\n",
    "location_percentage = users_with_location / total_users * 100\n",
    "bios_percentage = users_with_bios / total_users * 100\n",
    "both_percentage = users_with_both / total_users * 100\n",
    "\n",
    "# Modify the labels of the subsets to include the percentages\n",
    "v.get_label_by_id('10').set_text(f'{users_with_location}\\n({location_percentage:.1f}%)')\n",
    "v.get_label_by_id('01').set_text(f'{users_with_bios}\\n({bios_percentage:.1f}%)')\n",
    "v.get_label_by_id('11').set_text(f'{users_with_both}\\n({both_percentage:.1f}%)')\n",
    "\n",
    "# Add the total number of users to the title\n",
    "plt.title(f'Total users: {total_users}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Location (country) search using GeonamesCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc = geonamescache.GeonamesCache()\n",
    "all_cities = gc.get_cities()\n",
    "\n",
    "# Filter cities by country code\n",
    "cities = {k: v for k, v in all_cities.items() if v['countrycode'] == 'FR'}\n",
    "\n",
    "# Convert the cities dictionary to a list of tuples\n",
    "cities_list = [(city['name'], city['population']) for city in cities.values()]\n",
    "\n",
    "# Sort the list by population in descending order and take the first 1000\n",
    "biggest_cities = sorted(cities_list, key=lambda x: x[1], reverse=True)[:652]\n",
    "\n",
    "# Print the  biggest cities\n",
    "for city, population in biggest_cities:\n",
    "    print(city, population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract city names from biggest_cities list\n",
    "city_names = [city[0] for city in biggest_cities]\n",
    "\n",
    "# Add 'france' to the list of city names\n",
    "city_names.append('france')\n",
    "\n",
    "# Convert all city names to lowercase to ignore case\n",
    "city_names = [city.lower() for city in city_names]\n",
    "\n",
    "# Convert location column to lowercase to ignore case\n",
    "lang['location'] = lang['location'].str.lower()\n",
    "\n",
    "# Based on the location column, assign either country France or other \n",
    "# Apply the function to the 'location' column and assign the result to a new 'country' column\n",
    "lang['country'] = lang['location'].apply(lambda x: utils2.assign_country(x, city_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the intersections - bios and locations\n",
    "french_country_french_language = lang[(lang['country'] == 'France') & (lang['language'] == 'fr')].shape[0]\n",
    "french_country_other_language = lang[(lang['country'] == 'France') & (lang['language'] != 'fr')].shape[0]\n",
    "\n",
    "french_country = lang[lang['country'] == 'France'].shape[0]\n",
    "french_bio = lang[lang['language'] == 'fr'].shape[0]\n",
    "\n",
    "users_with_both = lang[(lang['location'].notna()) & (lang['description_cleantext'].notna())].shape[0]\n",
    "\n",
    "# Define the set sizes and the intersection sizes\n",
    "venn_labels = {'10': french_country_other_language, \n",
    "               '01': french_bio - french_country_french_language, \n",
    "               '11': french_country_french_language}\n",
    "\n",
    "# Calculate the total number of users\n",
    "total_users = users_with_both\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "venn = venn2(subsets=venn_labels, set_labels=(None, None))\n",
    "\n",
    "# Set 'French country' to red\n",
    "venn.get_patch_by_id('10').set_color('red')\n",
    "\n",
    "# Add a title\n",
    "plt.title(f'Users with bio and location: {total_users}')\n",
    "\n",
    "# Create the legend\n",
    "french_country_patch = mpatches.Patch(color='red', label='French country')  # Set 'French country' to red in the legend\n",
    "french_language_patch = mpatches.Patch(color=venn.get_patch_by_id('01').get_facecolor(), label='French bio')\n",
    "plt.legend(handles=[french_country_patch, french_language_patch], bbox_to_anchor=(1.05, 0.5), loc='center left')\n",
    "\n",
    "# Add the total count per category underneath each circle\n",
    "plt.text(-0.25, -0.5, f'Users in France: {french_country}', ha='center')\n",
    "plt.text(0.25, -0.55, f'French bios: {french_bio}', ha='center')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "#The numbers change abit each time the language detection is ran, because I did not set a seed and the language detection is probabilistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate dfs for the different categories\n",
    "french_bio_not_france = lang[(lang['language'] == 'fr') & (lang['country'] != 'France')]\n",
    "in_france_nofrenc_bio = lang[(lang['language'] != 'fr') & (lang['country'] == 'France')]\n",
    "bio_and_country = lang[(lang['language'] == 'fr') & (lang['country'] == 'France')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # how many rows in french_bio_not_france in the column location are NA?\n",
    "# french_bio_not_france['location'].isna().sum()\n",
    "\n",
    "# #print a selected interval of the rows in french_bio_not_france that are NA in location\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# french_bio_not_france[french_bio_not_france['location'].isna()][['description_cleantext', 'language', 'location', 'country']].iloc[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Write the language df to csv to save it in case something happens\n",
    "# bio_and_country.to_csv('/home/livtollanes/NewData/DataWrangling/inital_dfs/followers_bios_french.csv', sep=',', encoding='utf-8', index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create informative edgelists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An informative edgelist is created via some data wrangling. The informative edgelist contains the types of the markers.\n",
    "\n",
    "One for the no language data, and one for french users. Just in case I need to revisit something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non language edgelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the cleaned edge list (before language and location filtering)\n",
    "full_path1 = '/home/livtollanes/NewData/DataWrangling/inital_dfs/markers_followers_nolang.csv'\n",
    "req_cols = ['marker_id', 'follower_id']\n",
    "dtypes = {'marker_id': 'object',\n",
    "          'follower_id': 'object'}\n",
    "\n",
    "edgelist = pd.read_csv(full_path1, encoding='utf-8', dtype=dtypes, usecols=req_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add twitter name, type, and followers to the edge list for more informative plots at a later stage\n",
    "\n",
    "# Full path to the Excel file where types and names of brands are stored\n",
    "file_path = '/home/livtollanes/SocialMarkers/MarkersFrenchTwitter.xlsx'\n",
    "\n",
    "# Load the Excel file\n",
    "cats = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The excel file does not contain twitter ID. We thus need to merge cats with the marker metadata file in order to get names and ids in the same df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the marker bios and rename ID columns\n",
    "full_path = '/home/livtollanes/SocialMarkers/markers_bios_2023-05-19.csv'\n",
    "\n",
    "req_cols = ['twitter_name', 'id', 'tweets', 'followers']\n",
    "\n",
    "dtypes = {\n",
    "    'twitter_name': 'object',\n",
    "    'id': 'object',\n",
    "    'description': 'object',\n",
    "    'tweets': 'int64',\n",
    "    'followers': 'int64'}\n",
    "\n",
    "marker_ids = pd.read_csv(full_path, usecols=req_cols, dtype=dtypes)\n",
    "\n",
    "#rename the twittwer id column to follower id \n",
    "marker_ids.rename(columns={'id':'marker_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map marker_ids onto cats from marker:ids\n",
    "cat_id = cats.merge(marker_ids, on='twitter_name', how='left')\n",
    "cat_id = cat_id.drop_duplicates(subset='marker_id')\n",
    "\n",
    "#streamline cat_id and edge list. Only include ids in cat id that exist in edge list\n",
    "cat_id = utils2.streamline_IDs(edgelist, cat_id, 'marker_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_id_nodupes = cat_id.drop_duplicates()\n",
    "\n",
    "#Get the twitter_name column from cat_id into the edge list by merging on marker_id\n",
    "edgelist = edgelist.merge(cat_id[['marker_id', 'twitter_name', 'type', 'followers']], on='marker_id', how='left')\n",
    "\n",
    "#write edgelist to csv\n",
    "#edgelist.to_csv('/home/livtollanes/NewData/DataWrangling/inital_dfs/full_edgelist.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### French edgelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Creating an informative edgelist of french users and the brands they follow only\n",
    "2. Filtering to only include markers with at least 20 french followers. Make sure that we still only have followers that follow five brands or more.Also, update the french bio df after this added streamlining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the edgelist with type and initial filters applied. Not yet filtered for language and location\n",
    "full_path2 = '/home/livtollanes/NewData/DataWrangling/inital_dfs/full_edgelist.csv'\n",
    "req_cols2 = ['marker_id', 'follower_id', 'twitter_name', 'type', 'followers']\n",
    "dtypes2 = {'marker_id': 'object',\n",
    "          'follower_id': 'object',\n",
    "          'twitter_name': 'object',\n",
    "          'type': 'object',\n",
    "          'followers': 'int64',}\n",
    "\n",
    "edgelist = pd.read_csv(full_path2, encoding='utf-8', dtype=dtypes2, usecols=req_cols2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load french followers - to be used in the actual CA fitting process, and for updating the edgelist of markers and followers\n",
    "full_path2 = '/home/livtollanes/NewData/DataWrangling/inital_dfs/followers_bios_french.csv'\n",
    "\n",
    "req_cols = ['follower_id', 'screen_name', 'description', 'description_cleantext', 'location', 'language', 'country','timestamp_utc']\n",
    "\n",
    "dtypes = {\n",
    "    'follower_id': 'object',\n",
    "    'screen_name': 'object',\n",
    "    'description': 'object',\n",
    "    'description_cleantext': 'object',\n",
    "    'location': 'object',\n",
    "    'language': 'object',\n",
    "    'country': 'object',\n",
    "    'timestamp_utc': 'float64'\n",
    "}\n",
    "\n",
    "french_bios = pd.read_csv(full_path2, usecols=req_cols, dtype=dtypes, engine= 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The french edgelist is simply the edgelist from before minus the followers not existing in the french bio df\n",
    "french_edgelist = utils2.streamline_IDs(source= french_bios, df_tofilter=edgelist, column= 'follower_id')\n",
    "french_edgelist = french_edgelist.rename(columns={'followers': 'marker_followers'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_edgelist['marker_id'].nunique()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One brand was removed when making the french edgelist. Whioch one?\n",
    "pd.set_option('display.max_rows', None)\n",
    "first_rows1 = edgelist.drop_duplicates(subset='marker_id', keep='first')\n",
    "first_rows2 = french_edgelist.drop_duplicates(subset='marker_id', keep='first')\n",
    "\n",
    "\n",
    "first_rows1[~first_rows1['marker_id'].isin(first_rows2['marker_id'])] #SergioTacchiniA\n",
    "\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = edgelist[edgelist['marker_id'] == '882270183875915776'] \n",
    "\n",
    "# Print the number of rows\n",
    "print(len(filtered_df))\n",
    "\n",
    "\n",
    "#In the original edgelist (not only french users), there were 40 followers for sergio Tacchini. \n",
    "#When streamlining the edgelist to only include french people, this brand got removed entirely. These were all people categorised as non french in previpus steps.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Filter markers to have at least 20 french followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column of number of french followers to the edgelist\n",
    "\n",
    "# Add 'french_followers' column\n",
    "french_edgelist['french_followers'] = french_edgelist.groupby('marker_id')['marker_id'].transform('size')\n",
    "\n",
    "# Sort by 'french_followers' in descending order\n",
    "sorted_edgelist = french_edgelist.sort_values(by='french_followers', ascending=False)\n",
    "\n",
    "\n",
    "#Filter the french edgelist to only include markers that have more than n french followers \n",
    "sorted_edgelist_1, removed_info = utils2.min_french_followers(sorted_edgelist, 20)\n",
    "removed_info \n",
    "\n",
    "# This removed 11 initial brands "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure that we still only have followers that follow five brands or more\n",
    "sorted_edgelist_12 = utils2.filter_followers(df = sorted_edgelist_1, follower_id_column= 'follower_id', min_brands= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils2.summary_stats(sorted_edgelist_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new groupings for brands\n",
    "# Define the replacements\n",
    "replacements = {\n",
    "    'clubs de football': 'football clubs',\n",
    "    'magazine': 'information',\n",
    "    'media': 'information',\n",
    "    'party': 'information',\n",
    "    'sport': 'consumption',\n",
    "    'luxe vetements et malls': 'consumption',\n",
    "    'commerce': 'consumption',\n",
    "    'grande distribution': 'consumption',\n",
    "    'chain restaurants': 'consumption',\n",
    "    'universities': 'education',\n",
    "    'ecoles de commerce': 'education',\n",
    "    'Lyc√©es professionels': 'education'\n",
    "}\n",
    "\n",
    "# Replace the values\n",
    "sorted_edgelist_12.loc[:,'type2'] = sorted_edgelist_12['type'].replace(replacements)\n",
    "\n",
    "# Display the new DataFrame\n",
    "sorted_edgelist_12.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #Now write the edgelist with a follower threshold to a csv file\n",
    "#sorted_edgelist_12.to_csv('/home/livtollanes/NewData/DataWrangling/french_edgelist_marker_threshold.csv', sep=',', encoding='utf-8', index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update the french bios to include only followers that are in the new edgelist\n",
    "french_bios_updated = utils2.streamline_IDs(source = sorted_edgelist_12, df_tofilter = french_bios, column= 'follower_id')\n",
    "\n",
    "#french_bios_updated.to_csv('/home/livtollanes/NewData/DataWrangling/followers_bios_french_updated.csv', sep=',', encoding='utf-8', index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geotext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
