{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three data sets:\n",
    "- Markers' bios and metadata (markers_bios)\n",
    "- Followers' bios and metadata (followers_bios)\n",
    "- All brands and their followers (markers-followers)\n",
    "\n",
    "\n",
    "Step by step plan:\n",
    "1. Load the bios of followers, and the marker-follower file. \n",
    "    - Provide summary statistics of users and brands. How many brands do we have? How many followers? Any missing data, duplicates etc.?\n",
    "\n",
    "2. Filter on marker-follower df:\n",
    "    - Create a dictionary of counts brands per follower\n",
    "    - Remove users that follow less than 5 (or more) brands\n",
    "    - Continuously track numbers of users removed\n",
    "    - Match the Follower_Ids in the now filtered marker-follower df with the follower-bio df. As such, the follower bios will only include users that follow more than five brands. Subsequent filters will be on the correct users (up to date follower-bios).\n",
    "\n",
    "3. Do the filters on the follower-bios:\n",
    "    - Remove users with less than 25 followers\n",
    "    - Remove users with less than 100 tweets\n",
    "\n",
    "4. Filter based on language: keep only french accounts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import html\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# Third-party library imports\n",
    "import dask.dataframe as dd\n",
    "from joblib import Parallel, delayed\n",
    "from langdetect import detect_langs, LangDetectException, DetectorFactory\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import pickle\n",
    "import regex\n",
    "import seaborn as sns\n",
    "from unidecode import unidecode\n",
    "import gcld3\n",
    "import ftfy\n",
    "import importlib\n",
    "\n",
    "# Local application/library specific imports\n",
    "import utils2\n",
    "from utils2 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load files and summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the data files and rename ID columns\n",
    "importlib.reload(utils2)\n",
    "\n",
    "# Load markers-followers\n",
    "load_path = '/home/livtollanes/SocialMarkers'\n",
    "file = 'markers_followers_2023-05-19.csv'\n",
    "\n",
    "req_cols = ['id', 'follower_id']\n",
    "dtypes = {'id': 'object',\n",
    "          'follower_id': 'object'}\n",
    "\n",
    "markers_followers = utils2.fileloader(load_path, file, req_cols, dtypes)\n",
    "\n",
    "\n",
    "#rename the twittwer id column to follower id \n",
    "markers_followers.rename(columns={'id':'marker_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the followers bios and rename ID columns\n",
    "load_path = '/home/livtollanes/SocialMarkers'\n",
    "file = 'markers_followers_bios_2023-05-19.csv'\n",
    "\n",
    "req_cols = ['twitter_id', 'id', 'screen_name', 'description', 'location', 'tweets', 'followers', 'friends', 'likes', 'lists','timestamp_utc']\n",
    "\n",
    "dtypes = {\n",
    "    'twitter_id': 'object',\n",
    "    'id': 'object',\n",
    "    'screen_name': 'object',\n",
    "    'description': 'object',\n",
    "    'location': 'object',\n",
    "    'tweets': 'float64',\n",
    "    'followers': 'float64',\n",
    "    'friends': 'float64',\n",
    "    'witheld_in_countries': 'float64'\n",
    "}\n",
    "\n",
    "followers_bios = utils2.fileloader(load_path, file, req_cols, dtypes)\n",
    "\n",
    "\n",
    "\n",
    "#rename the twittwer id column to follower id \n",
    "followers_bios.rename(columns={'twitter_id':'follower_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (70666646, 11)\n",
      "\n",
      "Columns in DataFrame:  ['follower_id', 'id', 'screen_name', 'description', 'timestamp_utc', 'location', 'tweets', 'followers', 'friends', 'likes', 'lists']\n",
      "\n",
      "Number of unique values in 'follower_id':  70666646\n",
      "Number of duplicate values in 'follower_id':  0\n",
      "\n",
      "Number of unique values in 'id':  70642661\n",
      "Number of duplicate values in 'id':  23984\n",
      "Number of missing values in 'follower_id':  0\n",
      "Number of missing values in 'id':  23985\n",
      "Number of missing values in 'screen_name':  23986\n",
      "Number of missing values in 'description':  42027215\n",
      "Number of missing values in 'timestamp_utc':  23985\n",
      "Number of missing values in 'location':  47956041\n",
      "Number of missing values in 'tweets':  23985\n",
      "Number of missing values in 'followers':  23985\n",
      "Number of missing values in 'friends':  23985\n",
      "Number of missing values in 'likes':  23985\n",
      "Number of missing values in 'lists':  23985\n",
      "\n",
      "Number of duplicate rows:  0\n"
     ]
    }
   ],
   "source": [
    "#utils2.summary_stats(followers_bios, print_dtypes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (126345412, 2)\n",
      "\n",
      "Columns in DataFrame:  ['marker_id', 'follower_id']\n",
      "\n",
      "Number of unique values in 'follower_id':  70636295\n",
      "Number of duplicate values in 'follower_id':  55709117\n",
      "\n",
      "Number of unique values in 'marker_id':  236\n",
      "Number of duplicate values in 'marker_id':  126345176\n",
      "\n",
      "Number of missing values in each column:\n",
      "'marker_id':  0\n",
      "'follower_id':  0\n",
      "\n",
      "Number of duplicate rows:  2357493\n"
     ]
    }
   ],
   "source": [
    "# importlib.reload(utils2)\n",
    "# utils2.summary_stats(markers_followers, print_dtypes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         marker_id          follower_id\n",
      "81032594  25053299            100000025\n",
      "89256298  25053299            100000025\n",
      "79323950  25053299  1000001004220420096\n",
      "87547795  25053299  1000001004220420096\n",
      "79369517  25053299  1000001771266232320\n",
      "87593356  25053299  1000001771266232320\n",
      "79562687  25053299           1000001815\n",
      "87786516  25053299           1000001815\n",
      "79371019  25053299  1000002790238777352\n",
      "87594858  25053299  1000002790238777352\n"
     ]
    }
   ],
   "source": [
    "# Sort the DataFrame to ensure that duplicates are next to each other\n",
    "markers_followers_sorted = markers_followers.sort_values(by=list(markers_followers.columns))\n",
    "\n",
    "# Find duplicates in the sorted DataFrame\n",
    "duplicates = markers_followers_sorted[markers_followers_sorted.duplicated(keep=False)]\n",
    "\n",
    "# Print the first 10 rows of duplicates (5 pairs)\n",
    "print(duplicates.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the duplicates in markers_followers\n",
    "markers_followers.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123987919, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markers_followers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 30351 unique values in df1 that don't exist in df2.\n",
      "There are 0 unique values in df2 that don't exist in df1.\n"
     ]
    }
   ],
   "source": [
    "compare_column_values(followers_bios, markers_followers, 'follower_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filter the marker-follower df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Filter the marker-follower df:\n",
    "    - Remove users that follow less than 5 (or more) brands\n",
    "\n",
    "    - Continuously track numbers of users removed\n",
    "    \n",
    "    - Match the Follower_Ids in the now filtered marker-follower df with the follower-bio df. As such, the follower bios \n",
    "    will only include users that follow more than five brands. Subsequent filters will be on the correct users (up to date follower-bios)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove users that follow less than 5 brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66693204 followers follow less than 5 brands (94.42% of the total followers).\n",
      "After removing these followers, 3943091 followers are left (5.58% of the followers in the inputted df).\n"
     ]
    }
   ],
   "source": [
    "n = 5  # minimal number of brands followed required to be included in the analysis\n",
    "importlib.reload(utils2)\n",
    "markers_followers_5 = utils2.filter_followers(markers_followers, 'follower_id', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed brands: {'1059975643'}\n"
     ]
    }
   ],
   "source": [
    "#number of unique brands left\n",
    "\n",
    "# Get the unique marker_id values in the original and filtered DataFrames\n",
    "original_brands = set(markers_followers['marker_id'].unique())\n",
    "filtered_brands = set(markers_followers_5['marker_id'].unique())\n",
    "\n",
    "# Find the brands that are in the original DataFrame but not in the filtered DataFrame\n",
    "removed_brands = original_brands - filtered_brands\n",
    "\n",
    "# Print the removed brands\n",
    "print(\"Removed brands:\", removed_brands) #corresponds to \"Napapijiri97\", which kindof sounds like a fake profile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match the IDs in the filtered marker-follower df with the follower bio df, so that the follower bios only are for those who follow at least 5 brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique follower_id in source DataFrame: 3943091\n",
      "Number of unique follower_id in filtered DataFrame after filtering: 3943091\n",
      "Removed 66723555 rows from the DataFrame to be filtered.\n",
      "3943091 rows are left in the filtered DataFrame.\n"
     ]
    }
   ],
   "source": [
    "followers_bios_5 = utils2.streamline_IDs(source = markers_followers_5, df_tofilter= followers_bios, 'follower_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 unique values in df1 that don't exist in df2.\n",
      "There are 0 unique values in df2 that don't exist in df1.\n"
     ]
    }
   ],
   "source": [
    "compare_column_values(followers_bios_5, markers_followers_5, 'follower_id')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Do the filters on the follower-bios:\n",
    "- Remove users with less than 25 followers\n",
    "- Remove users with less than 100 tweets\n",
    "- Update the markers-followers df to match the now filtered bio df\n",
    "- Filter based on language: keep only french accounts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 2750559 rows.\n",
      "1192532 rows are left.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "followers_bios_fullfilter = utils2.filter_by_tweets_and_followers(followers_bios_5, min_followers= 25, min_tweets= 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, remove the follower_Ids in markers-followers that don't occur in the newly filtered  followers_bios_nd5_tweets_followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique follower_id in source DataFrame: 1192532\n",
      "Number of unique follower_id in filtered DataFrame after filtering: 1192532\n",
      "Removed 18992709 rows from the DataFrame to be filtered.\n",
      "9614122 rows are left in the filtered DataFrame.\n"
     ]
    }
   ],
   "source": [
    "markers_followers_fullfilter = utils2.streamline_IDs(source= followers_bios_fullfilter, df_tofilter=markers_followers_5, column='follower_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 unique values in df1 that don't exist in df2.\n",
      "There are 0 unique values in df2 that don't exist in df1.\n"
     ]
    }
   ],
   "source": [
    "compare_column_values(followers_bios_fullfilter, markers_followers_fullfilter , 'follower_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before writing ti csv, clean description column to avoid writing problems\n",
    "importlib.reload(utils2)\n",
    "\n",
    "followers_bios_fullfilter = utils2.process_description(followers_bios_fullfilter, 'description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (9614122, 2)\n",
      "\n",
      "Columns in DataFrame:  ['marker_id', 'follower_id']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of unique values in 'follower_id':  1192532\n",
      "Number of duplicate values in 'follower_id':  8421590\n",
      "\n",
      "Number of unique values in 'marker_id':  235\n",
      "Number of duplicate values in 'marker_id':  9613887\n",
      "\n",
      "Number of missing values in each column:\n",
      "'marker_id':  0\n",
      "'follower_id':  0\n",
      "\n",
      "Number of duplicate rows:  0\n"
     ]
    }
   ],
   "source": [
    "summary_stats(markers_followers_fullfilter, print_dtypes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #Now write the two dfs to csvs to save them in case something happens\n",
    "markers_followers_fullfilter.to_csv('/home/livtollanes/NewData/markers_followers_cleaned_nolang.csv', encoding='utf-8', index=False)\n",
    "\n",
    "followers_bios_fullfilter.to_csv('/home/livtollanes/NewData/followers_bios_cleaned_nolang3.csv', sep=',', encoding='utf-8', index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filter based on language: keep only french accounts\n",
    "- Use language recognition alorithms to filter the follower_bios. \n",
    "- We only want french language bios to be included\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load marker followers\n",
    "full_path1 = '/home/livtollanes/NewData/markers_followers_cleaned_nolang.csv'\n",
    "req_cols = ['marker_id', 'follower_id']\n",
    "dtypes = {'marker_id': 'object',\n",
    "          'follower_id': 'object'}\n",
    "\n",
    "markers_followers_clean = pd.read_csv(full_path1, encoding='utf-8', dtype=dtypes, usecols=req_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Loading the followers bios (with cleaned description column)\n",
    "full_path = '/home/livtollanes/NewData/followers_bios_cleaned_nolang3.csv'\n",
    "\n",
    "req_cols = ['follower_id', 'screen_name', 'description', 'description_cleantext', 'location', 'tweets', 'followers', 'friends', 'likes', 'lists','timestamp_utc']\n",
    "\n",
    "dtypes = {\n",
    "    'follower_id': 'object',\n",
    "    'screen_name': 'object',\n",
    "    'description': 'object',\n",
    "    'description_cleantext': 'object',\n",
    "    'location': 'object',\n",
    "    'tweets': 'float64',\n",
    "    'followers': 'float64',\n",
    "    'friends': 'float64'\n",
    "}\n",
    "\n",
    "follower_bios_cleaned3 = pd.read_csv(full_path, usecols=req_cols, dtype=dtypes, engine= 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_column_values(follower_bios_cleaned3, markers_followers_clean, 'follower_id')\n",
    "\n",
    "#The follower_ids are still streamlined, indicating that writing and reading of the cleaned dfs was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should probably do summary stats on the dfs again, to make sure that no strange things have happened during the cleaning process\n",
    "summary_stats(follower_bios_cleaned3, print_dtypes= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats(markers_followers_clean, print_dtypes= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes the language detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First language detection is with the langdetect package. This package is based on Google's language detection API. On medium, it is actually reported that this package may not perform very accurate on short or mixed lnaguage texts, which is exactly what we are deailing with in twitter bios. \n",
    "- Gcld3 mmight be better at handling short text inputs like bios. By reading the documentation, gcld3 seems to be more fitting for social media data language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils2)\n",
    "# Create a copy of the DataFrame for each function\n",
    "follower_bios_cleaned3_copy1 = follower_bios_cleaned3.copy()\n",
    "follower_bios_cleaned3_copy2 = follower_bios_cleaned3.copy()\n",
    "\n",
    "# Use the copied DataFrames in the functions\n",
    "lang = utils2.add_and_detect_language(follower_bios_cleaned3_copy1, 'description_cleantext', seed = 3, n_jobs=-1)\n",
    "lang2 = utils2.detect_language_gcld3(follower_bios_cleaned3_copy2, 'description_cleantext', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows = lang.shape[0]\n",
    "\n",
    "french_percent = (lang[lang['language'] == 'fr'].shape[0] / total_rows) * 100\n",
    "english_percent = (lang[lang['language'] == 'en'].shape[0] / total_rows) * 100\n",
    "unknown_percent = (lang[lang['language'] == 'unknown'].shape[0] / total_rows) * 100\n",
    "other_percent = 100 - french_percent - english_percent - unknown_percent\n",
    "\n",
    "print(\"French: \", french_percent, \"%\")\n",
    "print(\"English: \", english_percent, \"%\")\n",
    "print(\"Unknown: \", unknown_percent, \"%\")\n",
    "print(\"Other: \", other_percent, \"%\")\n",
    "\n",
    "nan_percent = (lang['description_cleantext'].isna().sum() / total_rows) * 100\n",
    "print(\"NaN in description_cleantext: \", nan_percent, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows = lang2.shape[0]\n",
    "\n",
    "french_percent = (lang2[lang2['language'] == 'fr'].shape[0] / total_rows) * 100\n",
    "english_percent = (lang2[lang2['language'] == 'en'].shape[0] / total_rows) * 100\n",
    "unknown_percent = (lang2[lang2['language'] == 'unknown'].shape[0] / total_rows) * 100\n",
    "other_percent = 100 - french_percent - english_percent - unknown_percent\n",
    "\n",
    "print(\"French: \", french_percent, \"%\")\n",
    "print(\"English: \", english_percent, \"%\")\n",
    "print(\"Unknown: \", unknown_percent, \"%\")\n",
    "print(\"Other: \", other_percent, \"%\")\n",
    "\n",
    "nan_percent = (lang2['description_cleantext'].isna().sum() / total_rows) * 100\n",
    "print(\"NaN in description_cleantext: \", nan_percent, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually creating test sets to compare the two language detection methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the langdetect df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = lang.sample(n=100, random_state=1)\n",
    "test2 = lang2.sample(n=100, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "test1[['screen_name', 'location','description_cleantext', 'language']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "true_labels = {\n",
    "    'MorganJewelers1': 'non_fr',\n",
    "    'LeighAnnTowne': 'non_fr',\n",
    "    'ALBrutel': 'NA',\n",
    "    'Fonkwadiour1': 'non_fr',\n",
    "    'JUSTINAGROSSO': 'NA',\n",
    "    'SarahRadif': 'non_fr',\n",
    "    'CarineMayol': 'NA',\n",
    "    'ManonMondou': 'fr',\n",
    "    'erickleangar': 'NA',\n",
    "    'elaineguimarae': 'non_fr',\n",
    "    'danyunes': 'fr',\n",
    "    'kuronapilled': 'non_fr',\n",
    "    'KiriYaji': 'non_fr',\n",
    "    'floriannisolle': 'fr',\n",
    "    'Naman93726237': 'fr', #french but from guinea\n",
    "    'doktor_vananga': 'non_fr', \n",
    "    'Slaiidh': \"non_fr\", #wrote non. could be another language\n",
    "    'StarRaull': \"non_fr\",\n",
    "    'SPeperstraete': 'non_fr',\n",
    "    'yorgaine_lyon69': 'fr',\n",
    "    'blinameta': 'non_fr',\n",
    "    'AnnaRus75': 'non_fr',\n",
    "    'BernTurner': 'NA',\n",
    "    'lauriebeebe2': 'non_fr',\n",
    "    'JeremyyPalazzo': 'NA',\n",
    "    'GalerieCoulange': 'fr',\n",
    "    'HaziqkaJ': 'NA',\n",
    "    'DaraCheick': 'non_fr', #but seems to belong to a french uni\n",
    "    'mmdlbdrny2': 'NA',\n",
    "    'blandineleonie': 'fr',\n",
    "    'CarlosO324': 'non_fr',\n",
    "    'joshuatoney3': 'non_fr',\n",
    "    'StephaneWayler': 'fr',\n",
    "    'JessieV_214': 'non_fr',\n",
    "    'Apartofsuzanne': 'non_fr',#but writes that she is french, and location is Paris\n",
    "    'katuxka1': 'NA',\n",
    "    'tamaitanya': 'NA',\n",
    "    'sulagnasays': 'non_fr',\n",
    "    'nicolesimon': 'non_fr',\n",
    "    'lunasitah': 'non_fr',\n",
    "    'ln971': 'fr',\n",
    "    'olive_cas': 'non_fr',\n",
    "    'Mercedeszatfc': 'NA',\n",
    "    'hady_saad': 'non_fr',\n",
    "    'GatienLeroux': 'fr',\n",
    "    'Meluxiam': 'non_fr',\n",
    "    'yulimadera10': 'non_fr',\n",
    "    'RyanBrossault': 'fr',\n",
    "    'FlotillaMRY': 'non_fr',\n",
    "    'love98_daniela': 'non_fr',\n",
    "    'jucag4115': 'NA',\n",
    "    'Bxwiz': 'non_fr',\n",
    "    'MLECOMTE': 'fr',\n",
    "    'DelgadoJanahi': 'non_fr',\n",
    "    'f1rmin': 'fr',\n",
    "    'SylvainWalczak': 'NA',\n",
    "    'SoyLigiawn': 'non_fr',\n",
    "    'javielo26': 'non_fr',\n",
    "    'CherrybangMUA': 'non_fr',\n",
    "    '1927_albert': 'non_fr',\n",
    "    'aayyoud': 'NA',\n",
    "    'princessivy28': 'non_fr',\n",
    "    'drumz420': 'NA',\n",
    "    'Cjruin': 'non_fr',\n",
    "    'xXallymayXx': 'non_fr',\n",
    "    'NaAutacaace': 'fr',\n",
    "    'RosaLuc15360022': 'non_fr',\n",
    "    'PatrikWinston': 'non_fr',\n",
    "    'mwa_sisqo': 'NA',\n",
    "    'vankrug': 'NA',\n",
    "    'miktoi': 'non_fr',\n",
    "    'cescoeco': 'non_fr',\n",
    "    'floo_ncy': 'NA',\n",
    "    'Quentin_1411': 'non_fr',\n",
    "    'Tvy_Tk': 'NA',\n",
    "    'AlbanLeneveu': 'NA',\n",
    "    'adh2311': 'non_fr',\n",
    "    'loaizayose': 'NA',\n",
    "    'mathilde_Fparis': 'fr',\n",
    "    'gouillardmichae': 'fr',\n",
    "    'RomainSprynski': 'fr',\n",
    "    'styledscience': 'non_fr',\n",
    "    'amhammadi': 'NA',\n",
    "    'AliRazaSharif2': 'NA',\n",
    "    'JahanLutz': 'fr',\n",
    "    'HERVEJEHL': 'fr',\n",
    "    'CatherineMSch': 'fr',\n",
    "    'PoliKeyCo': 'non_fr',\n",
    "    'HisGraceth': 'non_fr',\n",
    "    'verstagoat': 'non_fr',\n",
    "    'G_deLinares': 'fr', \n",
    "    'Granadechkou': 'fr',\n",
    "    'MommaDandine': 'NA',\n",
    "    'allionesolution': 'non_fr',\n",
    "    'Ross75016': 'fr',\n",
    "    'DorignyTheo': 'NA',\n",
    "    'nikkolasg1': 'non_fr',\n",
    "    'InesNau': 'fr',\n",
    "    'FrenchEmperior': 'non_fr',\n",
    "    'BekaEssick': 'non_fr'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_language(lang):\n",
    "    if lang == 'unknown':\n",
    "        return 'NA'\n",
    "    elif lang == 'fr':\n",
    "        return 'fr'\n",
    "    else:\n",
    "        return 'non_fr'\n",
    "\n",
    "test1['pred_lang'] = test1['language'].apply(label_language)\n",
    "test2['pred_lang'] = test2['language'].apply(label_language)\n",
    "\n",
    "test1['true_lang'] = test1['screen_name'].map(true_labels)\n",
    "test2['true_lang'] = test2['screen_name'].map(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Calculate metrics for test1\n",
    "print(\"Metrics for test1:\")\n",
    "print(classification_report(test1['true_lang'], test1['pred_lang']))\n",
    "print(\"Accuracy:\", accuracy_score(test1['true_lang'], test1['pred_lang']))\n",
    "\n",
    "# Calculate metrics for test2\n",
    "print(\"\\nMetrics for test2:\")\n",
    "print(classification_report(test2['true_lang'], test2['pred_lang']))\n",
    "print(\"Accuracy:\", accuracy_score(test2['true_lang'], test2['pred_lang']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Still lack the actual language filtering\n",
    "- Think of a more sophisticated way to locate french users than only using language. This method should capture french users that write in english, and in french, and avoid selecting users that are writing in french but are not actually french. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "10.1thesis",
   "language": "python",
   "name": "10.1thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
