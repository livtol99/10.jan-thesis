{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import html\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Third-party library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dask.dataframe as dd\n",
    "import psutil\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Local application/library specific imports\n",
    "import utils\n",
    "from utils import *\n",
    "\n",
    "\n",
    "\n",
    "from unidecode import unidecode\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # How much memory available?\n",
    "# def get_available_memory():\n",
    "#     return psutil.virtual_memory().available\n",
    "\n",
    "# available_memory = get_available_memory()\n",
    "# print(f\"Available memory: {available_memory / (1024 * 1024 * 1024)} GB\")\n",
    "\n",
    "\n",
    "# # check size of loaded df and print \n",
    "# def get_df_memory_usage(df):\n",
    "#     return df.info(memory_usage='deep')\n",
    "\n",
    "# get_df_memory_usage(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Filtering french brands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll aim to select only the french brands. \n",
    "- One approach is to use language detection. Via the gcd3 library (?)\n",
    "- Another approach could be to look at location\n",
    "\n",
    "\n",
    "Plan:\n",
    "- Load the df of the brands and their bios. \n",
    "- Filter\n",
    "- Now, filter the reduced follower df based on these final french brands\n",
    "\n",
    "- are all users now french ? (look at combination of language and location - potentially drop those that does not have any usable indicator)\n",
    "\n",
    "\n",
    "- Might need to manually inspect these later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter_id,id,screen_name,name,description,url,timestamp_utc,local_time,location,verified,protected,tweets,followers,friends,likes,lists,image,default_profile,default_profile_image,witheld_in_countries,witheld_scope\n",
      "\n",
      "3342215494,3342215494,titisanogo8,Titi sanogo,Je crois en DIEU et à mon travail j'y arriverai.....,,1435017944,2015-06-23T00:05:44,\"Ile-de-France, France\",0,0,6,44,733,91,0,https://pbs.twimg.com/profile_images/1249394390029742081/xuVolLn6_normal.jpg,1,0,,\n",
      "\n",
      "3115495713,3115495713,AndreDeybach,DEYBACH André,,,1427309108,2015-03-25T18:45:08,,0,0,0,1,40,0,0,https://pbs.twimg.com/profile_images/580803690757533697/pHNcCBLh_normal.jpg,1,0,,\n",
      "\n",
      "244075010,244075010,matttownley1985,Matt Townley,\"Hotelier, traveller, fan of all things hospitality, great food and fine wine! All views my own etc!!\",,1296220595,2011-01-28T13:16:35,\"Manchester, England\",0,1,2535,772,1264,1251,7,https://pbs.twimg.com/profile_images/928075998930681856/ZFXboKc3_normal.jpg,0,0,,\n",
      "\n",
      "2986463442,2986463442,alex_guevara90,Alex ,to all MI b**** what's up,,1421472910,2015-01-17T05:35:10,,0,0,12,8,118,172,0,https://pbs.twimg.com/profile_images/556324296038285313/Ev_NvKzl_normal.png,1,0,,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#reload(utils)\n",
    "importlib.reload(utils)\n",
    "\n",
    "#How is my data delimited?\n",
    "path = '/home/livtollanes/SocialMarkers'\n",
    "file = 'markers_followers_bios_2023-05-19.csv'\n",
    "\n",
    "utils.print_lines(path, file, 0,5)\n",
    "\n",
    "#The data is comma delimited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the markers_bios_2023-05-19.csv file\n",
    "path = '/home/livtollanes/SocialMarkers'\n",
    "file = 'markers_bios_2023-05-19.csv'\n",
    "\n",
    "req_cols = ['twitter_name', 'id', 'screen_name', 'description', 'location', 'tweets', 'followers', 'friends', 'likes', 'lists','timestamp_utc']\n",
    "\n",
    "dtypes = {'twitter_name':'object', \n",
    "          'id': 'float64',\n",
    "          'screen_name': 'object', \n",
    "          'description': 'object',\n",
    "          'location': 'object',\n",
    "          'tweets': 'float64',\n",
    "          'followers': 'float64',\n",
    "          'friends': 'float64',\n",
    "          'likes': 'float64',\n",
    "          'lists': 'float64',\n",
    "          'timestamp_utc': 'float64'}\n",
    "\n",
    "dfm = utils.fileloader(path, file, req_cols, dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Remove emojis, weird font, and detect language in descriptions\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Process the descriptions in the DataFrame\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m dfm \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_description\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Ssplit the DataFrame by language\u001b[39;00m\n\u001b[1;32m      7\u001b[0m dfm_french, dfm_other \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39msplit_by_language(dfm, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/10.jan-thesis/Code/utils.py:87\u001b[0m, in \u001b[0;36mprocess_description\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     85\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription_noems\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m bio: remove_emoji(bio) \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mnotnull(bio) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     86\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription_noems\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription_noems\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m bio: convert_to_regular_script(bio) \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mnotnull(bio) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 87\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdescription_noems\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbio\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetect_language\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbio\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munknown\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/miniconda3/envs/10.1thesis/lib/python3.11/site-packages/pandas/core/series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4640\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4758\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4762\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4764\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/10.1thesis/lib/python3.11/site-packages/pandas/core/apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/10.1thesis/lib/python3.11/site-packages/pandas/core/apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/miniconda3/envs/10.1thesis/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/10.1thesis/lib/python3.11/site-packages/pandas/core/algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1818\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/10.jan-thesis/Code/utils.py:87\u001b[0m, in \u001b[0;36mprocess_description.<locals>.<lambda>\u001b[0;34m(bio)\u001b[0m\n\u001b[1;32m     85\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription_noems\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m bio: remove_emoji(bio) \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mnotnull(bio) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     86\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription_noems\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription_noems\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m bio: convert_to_regular_script(bio) \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mnotnull(bio) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 87\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription_noems\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m bio: \u001b[43mdetect_language\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbio\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m bio\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/10.jan-thesis/Code/utils.py:70\u001b[0m, in \u001b[0;36mdetect_language\u001b[0;34m(bio)\u001b[0m\n\u001b[1;32m     68\u001b[0m DetectorFactory\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m LangDetectException:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/10.1thesis/lib/python3.11/site-packages/langdetect/detector_factory.py:130\u001b[0m, in \u001b[0;36mdetect\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    128\u001b[0m detector \u001b[38;5;241m=\u001b[39m _factory\u001b[38;5;241m.\u001b[39mcreate()\n\u001b[1;32m    129\u001b[0m detector\u001b[38;5;241m.\u001b[39mappend(text)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/10.1thesis/lib/python3.11/site-packages/langdetect/detector.py:136\u001b[0m, in \u001b[0;36mDetector.detect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    133\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Detect language of the target text and return the language name\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    which has the highest probability.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m probabilities:\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m probabilities[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlang\n",
      "File \u001b[0;32m~/miniconda3/envs/10.1thesis/lib/python3.11/site-packages/langdetect/detector.py:143\u001b[0m, in \u001b[0;36mDetector.get_probabilities\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_probabilities\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlangprob \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_detect_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_probability(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlangprob)\n",
      "File \u001b[0;32m~/miniconda3/envs/10.1thesis/lib/python3.11/site-packages/langdetect/detector.py:161\u001b[0m, in \u001b[0;36mDetector._detect_block\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_lang_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mngrams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_prob(prob) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONV_THRESHOLD \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mITERATION_LIMIT:\n",
      "File \u001b[0;32m~/miniconda3/envs/10.1thesis/lib/python3.11/site-packages/langdetect/detector.py:212\u001b[0m, in \u001b[0;36mDetector._update_lang_prob\u001b[0;34m(self, prob, word, alpha)\u001b[0m\n\u001b[1;32m    210\u001b[0m weight \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBASE_FREQ\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m xrange(\u001b[38;5;28mlen\u001b[39m(prob)):\n\u001b[0;32m--> 212\u001b[0m     prob[i] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m weight \u001b[38;5;241m+\u001b[39m lang_prob_map[i]\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Remove emojis, weird font, and detect language in descriptions\n",
    "\n",
    "# Process the descriptions in the DataFrame\n",
    "dfm = utils.process_description(dfm)\n",
    "\n",
    "# Ssplit the DataFrame by language\n",
    "dfm_french, dfm_other = utils.split_by_language(dfm, 'fr')\n",
    "\n",
    "# Finally, print information about the resulting DataFrames\n",
    "utils.print_df_info(dfm_french, 'dfm_french')\n",
    "utils.print_df_info(dfm_other, 'dfm_other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual inspection - did we miss any brands that are french?\n",
    "- This part was done by inspecting the data frame and looking up brands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The brands that were incorrectly detected as non french:\n",
    "\n",
    "- 21: Lafuma_France\n",
    "- 30: CarrefourFrance\n",
    "- 36: CasinoEnseigne\n",
    "- 37: Supermarche_G20\n",
    "- 38: VogueFrance\n",
    "- 39: FigaroMagazine\n",
    "- 41: LeMediaTv\n",
    "- 48: BFMTV\n",
    "- 92: TeleLoisirs\n",
    "- 94: ParisMatch\n",
    "- 96: Telerama\n",
    "- 101: EntMagazine\n",
    "- 106: OnzeMondial\n",
    "- 113: GQ_France\n",
    "- 119: LEXPRESS\n",
    "- 121: courrrierinter\n",
    "- 124: RCLens\n",
    "- 128: OL\n",
    "- 129: ognice\n",
    "- 131: StadeDeReims\n",
    "- 133: MontpellierHSC\n",
    "- 135: RCSA\n",
    "- 171: HECParis\n",
    "- 178: SciencesPo\n",
    "- 181: Univbordeaux\n",
    "- 182: UnivLyon1\n",
    "- 199: UniversiteCergy\n",
    "- 202: centralesupelec\n",
    "- 208: ENSAEparis\n",
    "- 215: esdes_BS\n",
    "- 221: LaCoudouliere\n",
    "- 226: LyceeProTissie\n",
    "- 227: PSVLaTournelle\n",
    "- 230: Decathlon\n",
    "- 231: Darty_Officiel\n",
    "- 233: Fnac\n",
    "- 236: AmazonFrance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add extra brands that were incorrectly defined as non-french:\n",
    "indices_to_change = [21, 30, 36, 37, 38, 39, 41, 48, 92, 94, 96, 101, 106, 113, 119, 121, 124, 128, 129, 131, 133, 135, 171, 178, 181, 182, 199, 202, 208, 215, 221, 226, 227, 230, 231, 233, 236]\n",
    "path_tosave = '/home/livtollanes/NewData'\n",
    "utils.add_extrabrands(indices_to_change, dfm_other, dfm_french, path_tosave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, in this section, we have been working with the bios of the markers. We deleted all rows that were from brands that were not french. The final output of this section includes two dfs: one for the french brands and one for other brands (only selected columns)\n",
    "\n",
    "In the next section, we will load the data with user bios and metadata. We will filter based on certain metrics, and also ignore all connections between followers and non french brands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filtering out irrelevant users "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this part, we want to remove users that don't follow enough brands. \n",
    "- This is to match the procedure of He and Tsvetkova (2023)\n",
    "- The purpose is to ensure we have enough information to generate SES estimates for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a look at the data (Of the markers and their followers)\n",
    "# path = '/home/livtollanes/SocialMarkers'\n",
    "# file = 'markers_followers_2023-05-19.csv'\n",
    "\n",
    "\n",
    "# def print_lines(path, file, start_line=0, end_line=10):\n",
    "#     with open(f\"{path}/{file}\", 'r') as f:\n",
    "#         for i in range(end_line):\n",
    "#             line = f.readline()\n",
    "#             if i >= start_line:\n",
    "#                 print(line)\n",
    "\n",
    "# print_lines(path, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the french metadata\n",
    "path = '/home/livtollanes/NewData'\n",
    "file = 'french_brands.csv'\n",
    "\n",
    "req_cols = ['twitter_name', 'id', 'screen_name', 'description', 'timestamp_utc', 'location', 'tweets', 'followers', 'friends', 'likes', 'lists', 'description_noems', 'language', 'corrected_language_country']\n",
    "\n",
    "dtypes = {'twitter_name':'object', \n",
    "          'id': 'float64',\n",
    "          'screen_name': 'object', \n",
    "          'description': 'object',\n",
    "          'location': 'object',\n",
    "          'tweets': 'float64',\n",
    "          'followers': 'float64',\n",
    "          'friends': 'float64',\n",
    "          'likes': 'float64',\n",
    "          'lists': 'float64',\n",
    "          'timestamp_utc': 'float64',\n",
    "          'description_noems': 'object',\n",
    "          'language': 'object',\n",
    "          'corrected_language_country': 'object'}\n",
    "\n",
    "french_marker_bios = utils.fileloader(path, file, req_cols, dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the list of marker ids and their followers' ids\n",
    "path = '/home/livtollanes/SocialMarkers'\n",
    "file = 'markers_followers_2023-05-19.csv'\n",
    "\n",
    "req_cols = ['id', 'follower_id']\n",
    "dtypes = {'id': 'float64',\n",
    "          'follower_id': 'float64'}\n",
    "\n",
    "markers_followers = utils.fileloader(path, file, req_cols, dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'iterrows'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(utils)\n\u001b[1;32m     11\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/livtollanes/NewData/french_markers_followers_2023-05-19.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 12\u001b[0m followers_per_brand_count \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_followers_per_brand_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(followers_per_brand_count))\n",
      "File \u001b[0;32m~/10.jan-thesis/Code/utils.py:221\u001b[0m, in \u001b[0;36mcreate_followers_per_brand_dict\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Create a dictionary of keys:brand_id and value: followers\u001b[39;00m\n\u001b[1;32m    219\u001b[0m followers_per_brand \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mset\u001b[39m)\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterrows\u001b[49m():\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# Add the follower id to the set of followers for this brand\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     followers_per_brand[row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39madd(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfollower_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Convert the sets to counts to see how many followers each brand has\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'iterrows'"
     ]
    }
   ],
   "source": [
    "#We only care about french brands\n",
    "\n",
    "# From the df with brand id and follower id, remove all rows with non french brands \n",
    "french_markers_followers = markers_followers[markers_followers['id'].isin(french_marker_bios['id'])]\n",
    "\n",
    "# Save the filtered data\n",
    "french_markers_followers.to_csv('/home/livtollanes/NewData/french_markers_followers_2023-05-19.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'iterrows'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(utils)\n\u001b[1;32m      3\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/livtollanes/NewData/french_markers_followers_2023-05-19.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m followers_per_brand_count \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_followers_per_brand_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Filter the dictionary to only include brands with more or equal to 10,000 followers\u001b[39;00m\n\u001b[1;32m      7\u001b[0m brands_10000_followers \u001b[38;5;241m=\u001b[39m {brand: followers \u001b[38;5;28;01mfor\u001b[39;00m brand, followers \u001b[38;5;129;01min\u001b[39;00m followers_per_brand_count\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m followers \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m9999\u001b[39m}\n",
      "File \u001b[0;32m~/10.jan-thesis/Code/utils.py:221\u001b[0m, in \u001b[0;36mcreate_followers_per_brand_dict\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Create a dictionary of keys:brand_id and value: followers\u001b[39;00m\n\u001b[1;32m    219\u001b[0m followers_per_brand \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mset\u001b[39m)\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterrows\u001b[49m():\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# Add the follower id to the set of followers for this brand\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     followers_per_brand[row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39madd(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfollower_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Convert the sets to counts to see how many followers each brand has\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'iterrows'"
     ]
    }
   ],
   "source": [
    "# How many followers are there per brand?\n",
    "importlib.reload(utils)\n",
    "filepath = '/home/livtollanes/NewData/french_markers_followers_2023-05-19.csv'\n",
    "followers_per_brand_count = utils.create_followers_per_brand_dict(filepath)\n",
    "\n",
    "# Filter the dictionary to only include brands with more or equal to 10,000 followers\n",
    "brands_10000_followers = {brand: followers for brand, followers in followers_per_brand_count.items() if followers > 9999}\n",
    "\n",
    "# Print the number of such brands\n",
    "print(len(brands_10000_followers))\n",
    "\n",
    "# Save the filtered data\n",
    "#french_markers_followers.to_csv('/home/livtollanes/NewData/french_markers_followers_2023-05-19.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "236\n",
      "176\n",
      "{9.8328592828117e+17, 9.101276052774952e+17, 9.05803918931886e+17}\n"
     ]
    }
   ],
   "source": [
    "#count the unique ids in french_marker_bios and in filtered_markers_followers\n",
    "print(french_marker_bios['id'].nunique())\n",
    "print(markers_followers['id'].nunique())\n",
    "print(french_markers_followers['id'].nunique())\n",
    "\n",
    "#print the unique ids that occur in french_marker_bios but not in filtered_markers_followers\n",
    "print(set(french_marker_bios['id']) - set(french_markers_followers['id']))\n",
    "\n",
    "# Get the unique ids that occur in dfm_french but not in filtered_markers_followers\n",
    "missing_ids = set(french_marker_bios['id']) - set(filtered_markers_followers['id'])\n",
    "\n",
    "# Filter french_marker_bios to only include rows with these ids\n",
    "missing_brands = french_marker_bios[french_marker_bios['id'].isin(missing_ids)]\n",
    "\n",
    "# Print the missing brands\n",
    "print(missing_brands)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obs: For three brands, existing in the french brands bio df, there are no data in the marker_follower data. Must inspect later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify the number of brands each unique follower follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionary of [keys: follower_Id, Value: Brand_id]\n",
    "Without loading the data into python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique values in the dictionary is 176.\n",
      "The number of keys in the dictionary is 16703791.\n",
      "First 5 items in the dictionary:\n",
      "('1.6598901997850952e+18', {'861574608.0', '25487201.0'})\n",
      "('1.2184586250285627e+18', {'25487201.0'})\n",
      "('1.603050305813418e+18', {'133663801.0', '25487201.0'})\n",
      "('1.65979527557623e+18', {'25487201.0'})\n",
      "('1.6598678949545697e+18', {'25487201.0'})\n"
     ]
    }
   ],
   "source": [
    "#Create a dictionary of [keys:follower_id, values: brands] based on the link between french brands and their followers\n",
    "importlib.reload(utils)\n",
    "\n",
    "filepath = '/home/livtollanes/NewData/french_markers_followers_2023-05-19.csv'\n",
    "brands_per_follower, brands_per_follower_count = utils.create_brands_per_follower_dict(filepath)\n",
    "\n",
    "#How many unique values (brands) are there in the dictionary?\n",
    "\n",
    "utils.inspect_dict(brands_per_follower, 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of keys that follow less than 5 IDs is 15069849, which is 90.22% of the total. Removing these leaves 9.78% of the data, or 1633942 users.\n",
      "Deleting rows ...\n",
      "First 5 items in the filtered dictionary:\n",
      "('1.5634751055898867e+18', {'34570323.0', '123564292.0', '33893706.0', '133663801.0', '19976004.0', '25487201.0'})\n",
      "('553059033.0', {'36383320.0', '76017958.0', '492648852.0', '34917842.0', '83864876.0', '1.0841564120900444e+18', '168619698.0', '94131301.0', '318695478.0', '63142684.0', '25487201.0', '3025757015.0'})\n",
      "('1.6596158075449344e+18', {'329683737.0', '125332632.0', '126244275.0', '33893706.0', '338891581.0', '47902100.0', '804263442.0', '25487201.0'})\n",
      "('1.6570728203445043e+18', {'329683737.0', '125332632.0', '114710148.0', '627377507.0', '33893706.0', '338891581.0', '50592815.0', '47902100.0', '804263442.0', '25487201.0', '318695478.0'})\n",
      "('7139582.0', {'156318405.0', '19900973.0', '63142684.0', '34570323.0', '492648852.0', '95455794.0', '133663801.0', '53029114.0', '19976004.0', '94544423.0', '17710206.0', '25487201.0', '19856081.0', '96090970.0'})\n"
     ]
    }
   ],
   "source": [
    "# How many people follow less than five brands? Remove these from the dictionary\n",
    "num_brands = 5\n",
    "num_items = 5\n",
    "filtered_brands_per_follower = utils.inspect_and_filter_followers(brands_per_follower, num_brands, num_items, remove=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the list of users that followe more than five brands to a .pkl file\n",
    "with open('/home/livtollanes/NewData/french_markers_follow5.pkl', 'wb') as f:\n",
    "     pickle.dump(filtered_brands_per_follower, f)\n",
    "\n",
    "\n",
    "# #load the pickle list from file\n",
    "# with open('/path/to/your/directory/french_markers_follow5.pkl', 'rb') as f:\n",
    "#      followers_filtered = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, filter the actual follower df based on the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in one data set\n",
    "path = '/home/livtollanes/SocialMarkers'\n",
    "file = 'markers_followers_bios_2023-05-19.csv'\n",
    "\n",
    "req_cols = ['twitter_id', 'id', 'screen_name', 'description', 'location', 'tweets', 'followers', 'friends', 'likes', 'lists','timestamp_utc']\n",
    "\n",
    "dtypes = {\n",
    "    'twitter_id': 'int64',\n",
    "    'id': 'float64',\n",
    "    'screen_name': 'object',\n",
    "    'description': 'object',\n",
    "    'location': 'object',\n",
    "    'tweets': 'float64',\n",
    "    'followers': 'float64',\n",
    "    'friends': 'float64',\n",
    "    'witheld_in_countries': 'float64'\n",
    "}\n",
    "\n",
    "frenchm_follower_bios = utils.fileloader(path, file, req_cols, dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary from the .pkl file\n",
    "with open('/home/livtollanes/NewData/french_markers_follow5.pkl', 'rb') as f:\n",
    "    followers_dict = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            twitter_id            id   screen_name  \\\n",
      "0           3342215494  3.342215e+09   titisanogo8   \n",
      "1           3115495713  3.115496e+09  AndreDeybach   \n",
      "7   738440598865088512  7.384406e+17   isusername1   \n",
      "28           912721573  9.127216e+08    Armelrichy   \n",
      "45  900384561640726530  9.003846e+17     SLIVIN001   \n",
      "\n",
      "                                          description  timestamp_utc  \\\n",
      "0   Je crois en DIEU et à mon travail j'y arrivera...   1.435018e+09   \n",
      "1                                                 NaN   1.427309e+09   \n",
      "7                                                 NaN   1.464893e+09   \n",
      "28                                                NaN   1.351526e+09   \n",
      "45                                                NaN   1.503503e+09   \n",
      "\n",
      "                 location  tweets  followers  friends  likes  lists  \n",
      "0   Ile-de-France, France     6.0       44.0    733.0   91.0    0.0  \n",
      "1                     NaN     0.0        1.0     40.0    0.0    0.0  \n",
      "7                     NaN     0.0        4.0    167.0    0.0    0.0  \n",
      "28                    NaN     0.0        2.0    168.0    0.0    0.0  \n",
      "45                    NaN     1.0        0.0    102.0    0.0    0.0  \n"
     ]
    }
   ],
   "source": [
    "# Convert the keys in followers_dict to integers\n",
    "follower_ids = [float(key) for key in followers_dict.keys()]\n",
    "\n",
    "# Filter the DataFrame to only include the rows where 'twitter_id' is in the follower_ids list\n",
    "filtered_df = frenchm_follower_bios[frenchm_follower_bios['twitter_id'].isin(follower_ids)]\n",
    "\n",
    "# Print the first few rows of the filtered DataFrame\n",
    "print(filtered_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1447587\n",
      "Number of columns: 11\n"
     ]
    }
   ],
   "source": [
    "# Print the number of rows in the DataFrame\n",
    "print(f\"Number of rows: {filtered_df.shape[0]}\")\n",
    "print(f\"Number of columns: {filtered_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all users that have sent less than 100 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after removing all with less than 100 tweets: 530919\n",
      "Number of rows removed: 916668\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame to only include users with more than 100 tweets\n",
    "filtered_df2 = filtered_df[filtered_df['tweets'] > 99]\n",
    "\n",
    "#print the number of rows in the filtered df\n",
    "print(f\"Number of rows after removing all with less than 100 tweets: {filtered_df2.shape[0]}\")\n",
    "\n",
    "#Print how many rows were removed\n",
    "print(f\"Number of rows removed: {filtered_df.shape[0] - filtered_df2.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after removing all with less than 25 followers: 445418\n",
      "Number of rows removed: 85501\n"
     ]
    }
   ],
   "source": [
    "#  Only include users with more than 25 followers\n",
    "filtered_df3 = filtered_df2[filtered_df2['followers'] > 24]\n",
    "\n",
    "# Print the number of rows in the filtered DataFrame   \n",
    "print(f\"Number of rows after removing all with less than 25 followers: {filtered_df3.shape[0]}\")\n",
    "\n",
    "# Print how many rows were removed\n",
    "print(f\"Number of rows removed: {filtered_df2.shape[0] - filtered_df3.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now write the filtered_df3 to a csv file\n",
    "# filtered_df3.to_csv('/home/livtollanes/NewData/XXX', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filterings\n",
    "- Follow at least five brands\n",
    "- sent at least 100 tweets\n",
    "- have at least 25 followers\n",
    "- sent at least five tweets in the first few months of the year the data was collected (maybe not relevant for this data - the twitter bios data does not contain the tweets. Only creation date for the profile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "10.1thesis",
   "language": "python",
   "name": "10.1thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
