{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import html\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Third-party library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dask.dataframe as dd\n",
    "import psutil\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Local application/library specific imports\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data from scratch - Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter_id,id,screen_name,name,description,url,timestamp_utc,local_time,location,verified,protected,tweets,followers,friends,likes,lists,image,default_profile,default_profile_image,witheld_in_countries,witheld_scope\n",
      "\n",
      "3342215494,3342215494,titisanogo8,Titi sanogo,Je crois en DIEU et à mon travail j'y arriverai.....,,1435017944,2015-06-23T00:05:44,\"Ile-de-France, France\",0,0,6,44,733,91,0,https://pbs.twimg.com/profile_images/1249394390029742081/xuVolLn6_normal.jpg,1,0,,\n",
      "\n",
      "3115495713,3115495713,AndreDeybach,DEYBACH André,,,1427309108,2015-03-25T18:45:08,,0,0,0,1,40,0,0,https://pbs.twimg.com/profile_images/580803690757533697/pHNcCBLh_normal.jpg,1,0,,\n",
      "\n",
      "244075010,244075010,matttownley1985,Matt Townley,\"Hotelier, traveller, fan of all things hospitality, great food and fine wine! All views my own etc!!\",,1296220595,2011-01-28T13:16:35,\"Manchester, England\",0,1,2535,772,1264,1251,7,https://pbs.twimg.com/profile_images/928075998930681856/ZFXboKc3_normal.jpg,0,0,,\n",
      "\n",
      "2986463442,2986463442,alex_guevara90,Alex ,to all MI b**** what's up,,1421472910,2015-01-17T05:35:10,,0,0,12,8,118,172,0,https://pbs.twimg.com/profile_images/556324296038285313/Ev_NvKzl_normal.png,1,0,,\n",
      "\n",
      "1519279241858330625,1519279241858330625,PilotDad87,Matt,Pilot. dad. husband. Catholic. Recteq master. 100 stroke golfer.,,1651059383,2022-04-27T11:36:23,\"Greenville, NC\",0,0,43,14,137,2069,0,https://pbs.twimg.com/profile_images/1519286870198345729/0woOjJ1P_normal.jpg,1,0,,\n",
      "\n",
      "3784597275,3784597275,cathybvaju,catherine,,,1443292646,2015-09-26T18:37:26,,0,0,8,49,256,21,0,https://pbs.twimg.com/profile_images/647843953417580544/CRIPCDj7_normal.jpg,1,0,,\n",
      "\n",
      "891453482070413312,891453482070413312,s18nanakaly,𝒮𝒶𝓃𝓃𝒶𝓇 ᥫ᭡,\"\"\"𝖧𝖺𝗇𝖽𝗌 𝗁𝖺𝗏𝖾 𝗍𝗁𝖾𝗂𝗋 𝗈𝗐𝗇 𝖼𝗈𝗇𝗏𝖾𝗋𝗌𝖺𝗍𝗂𝗈𝗇 \"\" ོ⠀ ⠀⠀⠀ ོ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ོ⠀⠀⠀⠀⠀ོ⠀⠀⠀ ོ⠀ ⠀⠀\",,1501374047,2017-07-30T00:20:47,Erbil,0,1,84,11,55,1289,0,https://pbs.twimg.com/profile_images/1553861976614469632/3hogj-tl_normal.jpg,1,0,,\n",
      "\n",
      "738440598865088512,738440598865088512,isusername1,isfirst,,,1464892933,2016-06-02T18:42:13,,0,0,0,4,167,0,0,https://abs.twimg.com/sticky/default_profile_images/default_profile_normal.png,1,1,,\n",
      "\n",
      "956867643998339074,956867643998339074,Wafaa20169665,Wafaa,💋😘☕💞😇,,1516969998,2018-01-26T12:33:18,\"Ain-Temouchent, Algérie\",0,0,0,2,111,0,0,https://pbs.twimg.com/profile_images/956869008610996224/r121pfIK_normal.jpg,1,0,,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#How is my data delimited?\n",
    "path = '/home/livtollanes/SocialMarkers'\n",
    "file = 'markers_followers_bios_2023-05-19.csv'\n",
    "\n",
    "def print_lines(path, file, start_line=0, end_line= 5):\n",
    "    with open(f\"{path}/{file}\", 'r') as f:\n",
    "        for i in range(end_line):\n",
    "            line = f.readline()\n",
    "            if i >= start_line:\n",
    "                print(line)\n",
    "\n",
    "print_lines(path, file)\n",
    "\n",
    "\n",
    "#The data is comma delimited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # How much memory available?\n",
    "# def get_available_memory():\n",
    "#     return psutil.virtual_memory().available\n",
    "\n",
    "# available_memory = get_available_memory()\n",
    "# print(f\"Available memory: {available_memory / (1024 * 1024 * 1024)} GB\")\n",
    "\n",
    "\n",
    "# # check size of loaded df and print \n",
    "# def get_df_memory_usage(df):\n",
    "#     return df.info(memory_usage='deep')\n",
    "\n",
    "# get_df_memory_usage(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Filtering french brands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll aim to select only the french brands. \n",
    "- One approach is to use language detection. Via the gcd3 library (?)\n",
    "- Another approach could be to look at location\n",
    "\n",
    "\n",
    "Plan:\n",
    "- Load the df of the brands and their bios. \n",
    "- Filter\n",
    "- Now, filter the reduced follower df based on these final french brands\n",
    "\n",
    "- are all users now french ? (look at combination of language and location - potentially drop those that does not have any usable indicator)\n",
    "\n",
    "\n",
    "- Might need to manually inspect these later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_file() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 19\u001b[0m\n\u001b[1;32m      5\u001b[0m req_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwitter_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscreen_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweets\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfollowers\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfriends\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlikes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlists\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp_utc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m dtypes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwitter_name\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      8\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscreen_name\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlists\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp_utc\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m---> 19\u001b[0m dfm \u001b[38;5;241m=\u001b[39m \u001b[43mload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: load_file() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#lod the markers_bios_2023-05-19.csv file\n",
    "path = '/home/livtollanes/SocialMarkers'\n",
    "file = 'markers_bios_2023-05-19.csv'\n",
    "\n",
    "req_cols = ['twitter_name', 'id', 'screen_name', 'description', 'location', 'tweets', 'followers', 'friends', 'likes', 'lists','timestamp_utc']\n",
    "\n",
    "dtypes = {'twitter_name':'object', \n",
    "          'id': 'float64',\n",
    "          'screen_name': 'object', \n",
    "          'description': 'object',\n",
    "          'location': 'object',\n",
    "          'tweets': 'float64',\n",
    "          'followers': 'float64',\n",
    "          'friends': 'float64',\n",
    "          'likes': 'float64',\n",
    "          'lists': 'float64',\n",
    "          'timestamp_utc': 'float64'}\n",
    "\n",
    "dfm = file_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove emojis, weird font, and detect language in descriptions\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "def remove_emoji(string):\n",
    "    return emoji.demojize(string, delimiters=(\"\", \"\"))\n",
    "\n",
    "def convert_to_regular_script(string):\n",
    "    return unidecode(string)\n",
    "\n",
    "def detect_language(bio):\n",
    "    try:\n",
    "        return detect(bio)\n",
    "    except LangDetectException:\n",
    "        return 'unknown'\n",
    "\n",
    "dfm['description_noems'] = dfm['description'].apply(lambda bio: remove_emoji(bio) if pd.notnull(bio) else '')\n",
    "dfm['description_noems'] = dfm['description_noems'].apply(lambda bio: convert_to_regular_script(bio) if pd.notnull(bio) else '')\n",
    "dfm['language'] = dfm['description_noems'].apply(lambda bio: detect_language(bio) if bio.strip() != '' else 'unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm_french = dfm[dfm['language'] == 'fr']\n",
    "\n",
    "#now create an \"other\" df with all the other languages\n",
    "dfm_other = dfm[dfm['language'] != 'fr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print how many rows are in the dfm_french df and add fstring\n",
    "print(f\"Number of rows in dfm_french: {dfm_french.shape[0]}\")\n",
    "\n",
    "#print how many rows are in the dfm_other df and add fstring\n",
    "print(f\"Number of rows in dfm_other: {dfm_other.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually adding faulty identified fr languages to the frencch df\n",
    "# Select the rows\n",
    "#selected_rows = dfm_other.loc[[1, 21, 28, 30, 36, 39, 41, 48, 78, 92, 94, 96, 101, 106, 119, 121, 124, 128, 129, 131, 133, 135, 181, 182, 199, 215, 230, 231, 233, 236,\n",
    "#226, 227, 221,208, 202, 178, 180, 171, 113, 38, 37]]\n",
    "\n",
    "# Print the selected rows for inspection\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#print(selected_rows[['twitter_name','description_noems']])\n",
    "\n",
    "# If the selected rows are correct, merge them into dfm_french and remove them from dfm_other\n",
    "dfm_french = pd.concat([dfm_french, selected_rows])\n",
    "dfm_other = dfm_other.drop([1, 21, 28, 30, 36, 39, 41, 48, 78, 92, 94, 96, 101, 106, 119, 121, 124, 128, 129, 131, 133, 135, 181, 182, 199, 215, 230, 231, 233, 236, 226, 227, 221,208, 202, 178, 180, 171, 113, 38, 37])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "print(dfm_french[['twitter_name']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filtering out irrelevant users "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this part, we want to remove users that don't follow enough brands. \n",
    "- This is to match the procedure of He and Tsvetkova (2023)\n",
    "- The purpose is to ensure we have enough information to generate SES estimates for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,cursor,follower_id\n",
      "\n",
      "415859364,,1655336804831174657\n",
      "\n",
      "415859364,,1659648141497454593\n",
      "\n",
      "415859364,,1525534139478310915\n",
      "\n",
      "415859364,,1659648883209674764\n",
      "\n",
      "415859364,,1659648836594458626\n",
      "\n",
      "415859364,,881616301\n",
      "\n",
      "415859364,,1618696506424303637\n",
      "\n",
      "415859364,,1659647872202055692\n",
      "\n",
      "415859364,,1659647973637357568\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Take a look at the data (Of the markers and their followers)\n",
    "# path = '/home/livtollanes/SocialMarkers'\n",
    "# file = 'markers_followers_2023-05-19.csv'\n",
    "\n",
    "\n",
    "# def print_lines(path, file, start_line=0, end_line=10):\n",
    "#     with open(f\"{path}/{file}\", 'r') as f:\n",
    "#         for i in range(end_line):\n",
    "#             line = f.readline()\n",
    "#             if i >= start_line:\n",
    "#                 print(line)\n",
    "\n",
    "# print_lines(path, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify the number of brands each unique follower follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionary of [keys: follower_Id, Value: Brand_id]\n",
    "Without loading the data into python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of keys:follower_id and value: brands\n",
    "brands_per_follower = defaultdict(set)\n",
    "\n",
    "# Open the CSV file\n",
    "with open('/home/livtollanes/SocialMarkers/markers_followers_2023-05-19.csv', 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        # Add the brand id to the set of brands for this follower\n",
    "        brands_per_follower[row['follower_id']].add(row['id'])\n",
    "\n",
    "\n",
    "# Convert the sets to counts to see how many brands each follower follows\n",
    "brands_per_follower_count = {follower_id: len(brands) for follower_id, brands in brands_per_follower.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique values in the dictionary is 236.\n"
     ]
    }
   ],
   "source": [
    "#How many unique values (brands) are there in the dictionary?\n",
    "\n",
    "unique_values = set(value for values in brands_per_follower.values() for value in values)\n",
    "\n",
    "# The number of unique values is the length of the set\n",
    "num_unique_values = len(unique_values)\n",
    "\n",
    "print(f\"The number of unique values in the dictionary is {num_unique_values}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of keys in the dictionary is 70636295.\n"
     ]
    }
   ],
   "source": [
    "# How many keys (follower_id) are there in the dictionary?\n",
    "num_keys = len(brands_per_follower_count)\n",
    "\n",
    "print(f\"The number of keys in the dictionary is {num_keys}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1655336804831174657', 4)\n",
      "('1659648141497454593', 3)\n",
      "('1525534139478310915', 3)\n",
      "('1659648883209674764', 1)\n",
      "('1659648836594458626', 1)\n"
     ]
    }
   ],
   "source": [
    "#inspect first key value pairs \n",
    "# Get an iterator over the dictionary's items\n",
    "items = iter(brands_per_follower_count.items())\n",
    "\n",
    "# Get the first 5 items\n",
    "for _ in range(5):\n",
    "    print(next(items))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of keys that follow less than 5 IDs is 66693204, which is 94.42% of the total. Removing these leaves 5.58% of the data, or 3943091 users.\n"
     ]
    }
   ],
   "source": [
    "# How many users follow less than num_brands brands?\n",
    "num_brands = 5\n",
    "\n",
    "# How many users follow less than num_brands brands?\n",
    "# Assuming brands_per_follower_count is your dictionary\n",
    "count = sum(1 for value in brands_per_follower_count.values() if value < num_brands)\n",
    "\n",
    "# Calculate the percentage\n",
    "percentage = (count / len(brands_per_follower_count)) * 100\n",
    "\n",
    "# Calculate the percentage of the remaining data\n",
    "remaining_percentage = 100 - percentage\n",
    "\n",
    "# Calculate the number of remaining users\n",
    "remaining_users = len(brands_per_follower_count) - count\n",
    "\n",
    "print(f\"The number of keys that follow less than {num_brands} IDs is {count}, which is {percentage:.2f}% of the total. Removing these leaves {remaining_percentage:.2f}% of the data, or {remaining_users} users.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1543191014579617793', {'415859364', '326359913', '88944305', '348379865', '38164846'})\n",
      "('1257705208592756737', {'60659498', '326359913', '44084633', '348379865', '60054156', '415859364'})\n",
      "('1301293067945807877', {'554957168', '88944305', '21915474', '44084633', '415859364'})\n",
      "('1644463375231885315', {'60659498', '88944305', '16096416', '44084633', '348379865', '60054156', '415859364'})\n",
      "('1233001848006729729', {'40448264', '83864876', '112754792', '33893706', '19976004', '88944305', '47902100', '133663801', '415859364'})\n"
     ]
    }
   ],
   "source": [
    "# excluding users that follow less than 5 brands in our full dict\n",
    "\n",
    "num_brands = 5\n",
    "\n",
    "# Filter the dictionary\n",
    "filtered_brands_per_follower = {follower_id: brands for follower_id, brands in brands_per_follower.items() if len(brands) >= num_brands}\n",
    "\n",
    "#inspect first key value pairs in filtered dict\n",
    "items = iter(filtered_brands_per_follower.items())\n",
    "\n",
    "# Get the first 5 items\n",
    "for _ in range(5):\n",
    "    print(next(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the list of users that followe more than five brands to a .pkl file\n",
    "with open('/home/livtollanes/NewData/followfive_filtered.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_brands_per_follower, f)\n",
    "\n",
    "\n",
    "#load the pickle list from file\n",
    "# with open('/path/to/your/directory/followers_filtered.pkl', 'rb') as f:\n",
    "#     followers_filtered = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, filter the follower df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Loading in one data set\n",
    "path = '/home/livtollanes/SocialMarkers'\n",
    "file = 'markers_followers_bios_2023-05-19.csv'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "req_cols = ['twitter_id', 'id', 'screen_name', 'description', 'location', 'tweets', 'followers', 'friends', 'likes', 'lists','timestamp_utc']\n",
    "\n",
    "dtypes = {\n",
    "    'twitter_id': 'int64',\n",
    "    'id': 'float64',\n",
    "    'screen_name': 'object',\n",
    "    'description': 'object',\n",
    "    'location': 'object',\n",
    "    'tweets': 'float64',\n",
    "    'followers': 'float64',\n",
    "    'friends': 'float64',\n",
    "    'witheld_in_countries': 'float64'\n",
    "}\n",
    "\n",
    "df = load_file(path, file, req_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary from the .pkl file\n",
    "with open('/home/livtollanes/NewData/followfive_filtered.pkl', 'rb') as f:\n",
    "    followers_dict = pickle.load(f)\n",
    "    \n",
    "#make a list of the keys in the dictionary\n",
    "followers = list(followers_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1543191014579617793: {'415859364', '326359913', '88944305', '348379865', '38164846'}\n",
      "1257705208592756737: {'60659498', '326359913', '44084633', '348379865', '60054156', '415859364'}\n",
      "1301293067945807877: {'554957168', '88944305', '21915474', '44084633', '415859364'}\n",
      "1644463375231885315: {'60659498', '88944305', '16096416', '44084633', '348379865', '60054156', '415859364'}\n",
      "1233001848006729729: {'40448264', '83864876', '112754792', '33893706', '19976004', '88944305', '47902100', '133663801', '415859364'}\n"
     ]
    }
   ],
   "source": [
    "# Convert the dictionary items to a list\n",
    "items = list(followers_dict.items())\n",
    "\n",
    "# Get the first few entries\n",
    "first_entries = items[:5]\n",
    "\n",
    "# Print the first few entries\n",
    "for key, value in first_entries:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   twitter_id            id      screen_name  \\\n",
      "44884750  1233001848006729729  1.233002e+18  OmarKHA95647383   \n",
      "\n",
      "                                description  timestamp_utc location  tweets  \\\n",
      "44884750  ‏‏كل أرض يرفع فيها الآذان هي وطني   1.582806e+09      NaN   248.0   \n",
      "\n",
      "          followers  friends   likes  lists  \n",
      "44884750      193.0   3862.0  4242.0    0.0  \n"
     ]
    }
   ],
   "source": [
    "# Search for 1233001848006729729 in 'twitter_id' column\n",
    "mask = df['twitter_id'] == 1233001848006729729\n",
    "\n",
    "# Use the mask to select the rows where 'twitter_id' is 1233001848006729729\n",
    "matching_rows = df[mask]\n",
    "\n",
    "# Print the matching rows\n",
    "print(matching_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            twitter_id            id    screen_name  \\\n",
      "0           3342215494  3.342215e+09    titisanogo8   \n",
      "1           3115495713  3.115496e+09   AndreDeybach   \n",
      "7   738440598865088512  7.384406e+17    isusername1   \n",
      "28           912721573  9.127216e+08     Armelrichy   \n",
      "35            30797693  3.079769e+07  AVMGDIGITALHD   \n",
      "\n",
      "                                          description  timestamp_utc  \\\n",
      "0   Je crois en DIEU et à mon travail j'y arrivera...   1.435018e+09   \n",
      "1                                                 NaN   1.427309e+09   \n",
      "7                                                 NaN   1.464893e+09   \n",
      "28                                                NaN   1.351526e+09   \n",
      "35  THE NEW DIGITAL STATION!!! \\nfollowed by @ROCN...   1.239593e+09   \n",
      "\n",
      "                 location   tweets  followers  friends  likes  lists  \n",
      "0   Ile-de-France, France      6.0       44.0    733.0   91.0    0.0  \n",
      "1                     NaN      0.0        1.0     40.0    0.0    0.0  \n",
      "7                     NaN      0.0        4.0    167.0    0.0    0.0  \n",
      "28                    NaN      0.0        2.0    168.0    0.0    0.0  \n",
      "35                    NaN  73584.0      777.0   1504.0  250.0   12.0  \n"
     ]
    }
   ],
   "source": [
    "# Convert the keys in followers_dict to integers\n",
    "follower_ids = [int(key) for key in followers_dict.keys()]\n",
    "\n",
    "# Filter the DataFrame to only include the rows where 'twitter_id' is in the follower_ids list\n",
    "filtered_df = df[df['twitter_id'].isin(follower_ids)]\n",
    "\n",
    "# Print the first few rows of the filtered DataFrame\n",
    "print(filtered_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 3943091\n",
      "Number of columns: 11\n"
     ]
    }
   ],
   "source": [
    "# Print the number of rows in the DataFrame\n",
    "print(f\"Number of rows: {filtered_df.shape[0]}\")\n",
    "print(f\"Number of columns: {filtered_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all users that have sent less than 100 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "#check the dtype ow tweets in filtered df\n",
    "print(filtered_df['tweets'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after removing all with less than 100 tweets: 1381356\n",
      "Number of rows removed: 2561735\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame to only include users with more than 100 tweets\n",
    "filtered_df2 = filtered_df[filtered_df['tweets'] > 99]\n",
    "\n",
    "#print the number of rows in the filtered df\n",
    "print(f\"Number of rows after removing all with less than 100 tweets: {filtered_df2.shape[0]}\")\n",
    "\n",
    "#Print how many rows were removed\n",
    "print(f\"Number of rows removed: {filtered_df.shape[0] - filtered_df2.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after removing all with less than 25 followers: 1192532\n",
      "Number of rows removed: 188824\n"
     ]
    }
   ],
   "source": [
    "#  Only include users with more than 25 followers\n",
    "filtered_df3 = filtered_df2[filtered_df2['followers'] > 24]\n",
    "\n",
    "# Print the number of rows in the filtered DataFrame   \n",
    "print(f\"Number of rows after removing all with less than 25 followers: {filtered_df3.shape[0]}\")\n",
    "\n",
    "# Print how many rows were removed\n",
    "print(f\"Number of rows removed: {filtered_df2.shape[0] - filtered_df3.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most recent timestamp: 1684644667.0\n",
      "Most recent datetime: 2023-05-21 04:51:07\n",
      "Oldest timestamp: 1153057347.0\n",
      "Oldest datetime: 2006-07-16 13:42:27\n"
     ]
    }
   ],
   "source": [
    "#Find the most recent timestamp\n",
    "most_recent_timestamp = filtered_df3['timestamp_utc'].max()\n",
    "print(f\"Most recent timestamp: {most_recent_timestamp}\")\n",
    "\n",
    "#convert the timestamp to a datetime object\n",
    "most_recent_datetime = datetime.fromtimestamp(most_recent_timestamp)\n",
    "print(f\"Most recent datetime: {most_recent_datetime}\")\n",
    "\n",
    "\n",
    "#Do the same for the oldest timestamp\n",
    "oldest_timestamp = filtered_df3['timestamp_utc'].min()\n",
    "print(f\"Oldest timestamp: {oldest_timestamp}\")\n",
    "\n",
    "#convert the timestamp to a datetime object\n",
    "oldest_datetime = datetime.fromtimestamp(oldest_timestamp)\n",
    "print(f\"Oldest datetime: {oldest_datetime}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now write the filtered_df3 to a csv file\n",
    "# filtered_df3.to_csv('/home/livtollanes/NewData/filtered_follower_bios_.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filterings\n",
    "- Follow at least five brands\n",
    "- sent at least 100 tweets\n",
    "- have at least 25 followers\n",
    "- sent at least five tweets in the first few months of the year the data was collected (maybe not relevant for this data - the twitter bios data does not contain the tweets. Only creation date for the profile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "10.1thesis",
   "language": "python",
   "name": "10.1thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
